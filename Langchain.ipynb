{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4df7225",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01fff7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e42cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "show_markdown_response = lambda response: display(Markdown(response.content))  # noqa: E731\n",
    "show_markdown_text = lambda text: display(Markdown(text))  # noqa: E731\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0622ab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mxbai-embed-large:latest\n",
      "granite-embedding:278m\n",
      "gemma3:latest\n",
      "qwen3:latest\n",
      "qwen2.5-coder:7b\n",
      "hf.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Uncensored-i1-GGUF:Q4_K_M\n"
     ]
    }
   ],
   "source": [
    "def show_ollama_models():\n",
    "    for i in ollama.list().models:\n",
    "        print(i.model)\n",
    "show_ollama_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d299cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma = ChatOllama( model=\"gemma3:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "805e1dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"Explain to people like they are five year olds.\"\"\"\n",
    "usr_prompt = \"\"\"What is Langchain?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341322ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "messages = [\n",
    "    SystemMessage( content=sys_prompt ),\n",
    "    HumanMessage( content= usr_prompt)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac7aaf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = gemma.invoke(messages)\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7b13c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd31e567",
   "metadata": {},
   "source": [
    "# Chains, Prompts, Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a108960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c48a01e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a helpful assistant that explains AI topics. Given the following input:\n",
    "{topic}\n",
    "Provide an explanation of the given topic\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3b0bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08d5ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e900118",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\n",
    "    {\"topic\" : \"What is a chain in lanchain?\" }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28057c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, let’s break down what a “chain” is in the context of Launchchain.\n",
       "\n",
       "**Launchchain’s Chains: The Building Blocks of Decentralized Applications**\n",
       "\n",
       "In Launchchain, a \"chain\" isn't just a blockchain in the traditional sense like Ethereum or Solana. It’s a *purpose-built, modular, and scalable environment* designed to facilitate the development and deployment of decentralized applications (dApps). Think of it as a highly customizable and efficient engine for building dApps.\n",
       "\n",
       "Here's a breakdown of what a Launchchain chain *is*:\n",
       "\n",
       "1. **Modular Infrastructure:** At its core, a Launchchain chain is built on a modular architecture. This means it's broken down into distinct components, each handling a specific function. These components include:\n",
       "\n",
       "   * **Data Layer:**  Handles the storage and retrieval of application data. Launchchain utilizes a sharded data layer, meaning data is distributed across multiple nodes, significantly increasing capacity and speed.\n",
       "   * **Computation Layer:**  This is where the actual logic of your dApp runs – smart contracts, or logic written in languages like Rust or Go.\n",
       "   * **Consensus Layer:**  This layer handles the agreement among nodes on the validity of transactions and the state of the chain. Launchchain uses a unique Delegated Proof-of-Stake (DPoS) consensus mechanism, chosen for its efficiency and scalability.\n",
       "   * **User Interface Layer:** Handles the presentation of the chain to users.\n",
       "\n",
       "2. **Scalability & Performance:** Launchchain's architecture is specifically designed for scalability. The sharded data layer and DPoS consensus are key to this. It's designed to handle a massive number of transactions per second, which is crucial for applications like DeFi and gaming.\n",
       "\n",
       "3. **Customization & Flexibility:**  This is a *critical* difference from many traditional blockchains. Launchchain chains are highly customizable. Developers can:\n",
       "    * **Choose their programming language:** Rust and Go are favored, offering performance and security.\n",
       "    * **Adjust parameters:** Parameters like block size, transaction fees, and consensus settings can be tweaked to optimize performance for specific dApp needs.\n",
       "    * **Select specific modules:**  Developers can choose precisely the components they require, avoiding unnecessary overhead.\n",
       "\n",
       "4. **Chains as Isolated Environments:**  Importantly, different Launchchain chains are *isolated*. This means that a dApp deployed on one chain doesn’t automatically interact with dApps on other Launchchain chains. This allows for greater security and control.\n",
       "\n",
       "\n",
       "**In Simple Terms:**\n",
       "\n",
       "Imagine you're building a factory. A traditional blockchain is like a huge, all-in-one factory. Launchchain chains are like smaller, specialized factories – one for processing data, one for running calculations, and so on. You can combine these specialized factories to build a complex product (your dApp) efficiently.\n",
       "\n",
       "**Resources for Further Learning:**\n",
       "\n",
       "*   **Launchchain Website:** [https://launchchain.io/](https://launchchain.io/) - This is the best place to start!\n",
       "*   **Launchchain Documentation:** [https://launchchain.io/docs](https://launchchain.io/docs)\n",
       "\n",
       "Do you want me to delve deeper into a specific aspect of Launchchain chains, such as:\n",
       "\n",
       "*   The Delegated Proof-of-Stake consensus mechanism?\n",
       "*   How customization works?\n",
       "*   The benefits of the modular architecture?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_markdown_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de0f4027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=8BV9TW490nQ&t=533s\" , \n",
    "    add_video_info = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba598aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "855b7e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"in this video you'll learn everything you need to know about Lang chain in seven easy steps I've broken down the framework into key components and here's an overview of the image I'm going to try to put in your mind in the next 40 minutes there is a lot to unpack but I'll walk you through the components one by one and I've included coding examples with every step I'll start off by asking why length chain and I'll give you a reason why you want to learn this and an overview of the framework then I'll show you how to build chains using prompts and ERS and we'll dive into the runable protocol and Lang chain expression language then we'll look at Splitters and retrievers and we'll build a rag chain finally I'll walk you through how to use tools and how to build an agent with access to Tools in this video I'll keep my focus on the core Lang chain framework and build the foundation for advanced use cases so I'll save Lang graph Lang Smith and Lang surf for separate videos if you're new to Lang chain this tutorial will serve as a quick start and if you're already already familiar with the framework you can use the tutorial to tie up some loose ends I'll share a link to this interactive map of Lang chain to the code and to a written tutorial below this video let's get started I'm going to start off by giving you an overview of the linkchain framework what is it why would you want to use it and then I'm going to show you how to get started developing in a collab notebook so what is Lang chain Lang chain is an open open source framework for developing llm applications an engine allows developers to combine llms with external sources of computation and data and the framework further simplifies the deployment of the llm applications by providing the needed tools such as tools for inspection monitoring and evaluation the deployment and monitoring tools are part of Lang Smith and Lang serf I'm not going to cover that here I'm going to cover the core Library so Lang chain combines llms with external sources of computation and data how does that work suppose that a user wants to ask a question related to some documents or some data that the user has access to now this question will be sent to the llm just like when you're interacting with chat DBT but you also need to fetch the context needed to answer the question from your own data or your own documents and to do that a vector representation of the initial question from the user is used to fetch all the relevant context needed to answer the question typically from a vector database this context is then fed to the llm alongside with the initial question and now the llm has everything it needs to answer the question in the right context and the magical thing about this is that we can do more than just answer the question we can actually take actions based on the input from the end user and we can do that because Lang chain allows us to build agents out of llms and agents are created by by combining llms with tools this will give the llms access to external sources of computation and data so what can we build with Lang chain when it comes to the types of applications you can build with llms this guise the limit from what I see most llm applications that have been built over the last year fall into one or three categories chatbots racun applications or agent systems so the chat BS are nothing new they're used for marketing customer support different types of education and creating assistance what is new is that these chat Bots can now be made more intelligent using llms then we have the rag Q&A applications and this is the magic that Lang chain and llama index made people aware of a year ago so these are the applications that are used for summarizing large documents doing data analysis and generating code by referencing repositories and then we have what we can call agent system both multi-agent systems and systems where we Loop humans into complex interactions with agents and this is what Lang graph is used for and Lang graph is a rather new component in the Lang chain ecosystem the number of potential applications of agent systems is virtually Limitless they're being implemented for Supply Chain management Revenue optimization operations management and so on all right so let's zoom out a bit and have a look at why you want to learn langing so so there are at least two good reasons for learning Lang chain so first of all Lang chain facilitates the transitioning to the next generation of programming languages and what do I mean by that so if we have a look at programming language hierarchies this is what it typically looks like so at the bottom of the pyramid you will have machine language on top of that you will have Assembly Language then you have the higher level languages those are the languages that you are used to program coming in so python Java SQL R C++ and so on and we've been moving up the pyramid ever since programming became a thing and right now we are at the final step the highest level of all languages which is natural language because with the llms we don't have to write code anymore we can have the llms write code for us by giving natural language as an input and the development of these pipelines that allows us to turn natural language into programs is exactly what Lang chain facilitates so we can create pipelines or chains as they are called that allows us to input natural language and get something of value in return like a report an analysis or computer program and with the current generation of best-in-class llms we can already write executable code code that works so this is not something that belongs to the Future it's already here now the second reason you you might want to use Lang chain is really not Lang chain itself but what's being built on top of Lang chain which is Lang graph now Lang chain was late to the multi-agent game you had autogen you had crew AI before that but what langra can do is really remarkable for instance Lang graph allows us to build complex interactions between agents and humans and this is needed especially in big organizations where you have to guard the access to certain files and so on but Lang graph also allows us to build multi-agent systems like autogen or crew II and the use cases for this are becoming more clear every day we need to be able to set up arbitrary communication patterns between agents we also need to do divide and conquer just like humans we can't have an llm do everything so we want to build specialized agents and we want them to be able to perform tasks in an asynchronous way all right let's get started developing the first thing you want to do is you want to fetch the API key from the llm service you want to use so in this tutorial I'm going to use openi and anthropic but Lang chain supports a wide array of llm providers so you can just choose your favorite one once you have grabbed your API key you want to put it in a EnV file all right so here on the collab notebook I have already installed the needed libraries here I'm going to install python. EnV to fetch API keys from the EnV file I'm going to install Lang chain of course Lang chain Community Omi and anthropic and I have uploaded the EnV file from my local machine to the cab notebook and now I'm ready to load the environment variables so I'm loading the API keys and once I've done that I can use chat anthropic and chat Omi to connect to anthropic and Omi and then I'm going to make a test call to GPT 4 and ask what is Lang chain and I simply make an LM call by using the invoke method and here we have an answer from gbt 4 this is very similar to what you would do if you were just calling gbt 4 directly through the open API and of course we can just switch out the llm now I'm calling clot 3 and here I also get a response so I can also easily prpt the llm with a system message and a user message at the same time and I'll just send the two messages in a list to the llm and this will allow you to do some basic prompt engineering but we can do a lot more advanced stuff than this once we start utilizing prom templates and building chains all right so let's dive into chains prompts and loaders and build our first chain I'm going to walk you through what a chain is and give you an analogy to think of then I'm going to work you through prom templates and how to use document loaders so a chain in Lang chain is a sequence of interconnected components that process a user's query utilizing one or more llms to generate and deliver some valuable output and I'm going to get into what those components are in a bit but let me just give you examples of the valuable output that we can generate with the chain so examples of valuable output could be an answer to a specific question it could be a summary of some document it could be a recommendation insights from data a piece of executable code or some Json for an API call and with the chains we can automate the generation of this value now what about the components the components of a chain are typically proms llms or what we call chat models output passes that passes the output of the llm tools that allow the llms to extract additional information from apis or run some code this is what turns llms into agents and general functions and in Lang chain these components are so-called runnables that can be chained together to produce a pipeline and this is one way to think of chains as pipelines in the world of llms so an analogy would be a data pipeline in a data pipeline we typically input some raw data and we get some nice clean data out in a table format and the data cleaning and transformation happening in the pipeline is valuable similarly in an llm chain we input a query and we get some valuable output in return and here the query is turned into value using llm calls functions and additional data and just like with data pipelines we can automate this process with cicd continuous integration and continuous delivery and this is what Lang chain as a framework can help you do so not only can you build the chains but you can also test the chains deploy them monitor the chains and evaluate the output and this can be done with Lang Smith and Lang surf so Lang chain has several built-in modules or classes that helps you create chains efficiently and one of those modules are prompt templates so a prompt for an llm is a set of instructions provided by a user to guide the model's response and promt templates in Lang chain are predefined prompt recipes these recipes are very helpful when you want to create chains fast another feature of Lang chain is that you have so-called document loaders a document is a piece of text and Associated metadata and the Lang chain document loaders provide a load method for loading data as documents from a source into a chain now let's head over to a collab notebook and set up a simple chain I'm going to import prompt template from Lang chain prompts and then I'm going to formulate the prompt to the llm as a string with a placeholder in this case with the variable topic and then I set up the prompt using the prompt template I just imported from Lang chain an input variable that I'm going to call topic and then the string the template itself and this is a simple example of a prompt that now allows me to comp OS a chain and I do that using the pipe operator so I can take the prompt and the language model LL mgbt 4 and assemble a chain using those two components and now I can invoke the chain with an input variable in this case topic so this was a very simple example of assembling a chain using the pipe operator let's try to make this a bit more advanced I'm going to create a chain that will transcribe one of my YouTube videos so I'm going to pip install the YouTube transcript API and then I can use one of the loaders that is spilled into L chain the YouTube loader so I'll import YouTube loader from Lang chain Community document loaders and then I can simply pass the URL to one of my videos or any video to the loader and have the loader extract the transcript from the video now if we have a look at what the loader extracts you can see that we get a list of documents if you want the transcript you can simply extract the page content from the documents or you can simply pass the documents to the chain directly so fetching the raw transcript would be as simple as just fetching the single document in this list and then extracting the page content all right so let's set up a chain that will summarize YouTube videos given a transcript so here I'm going to inject the video transcript into the prom template so this is going going to be my input variable and then I just set up the chain like I did before with the pipe operator and I'm going to invoke it with the video transcript and note that I can just pass the documents I don't have to extract the raw transcript and here we have a nice summary of my YouTube video so Lang chain also has a number of build-in chains that you can use for stuff like this for instance the create stuff documents chain and this chain will take a list of documents and pass them all in as context in the prom template pretty much what we did before just without using the buil-in chain this was just a very basic introduction to building chains now in order to build more advanced chains we need to understand Lang chains runnable protocol and Lang chain expression language in more detail let's now try to dive deeper into Lang chain expression language and the runnable protocol the Lang chain expression language simplifies building complex chains from basic components and we do that using the pipe operator that allows us to chain different components together feeding the output from one component to the next so a simple example of a chain composed in this way would be a prompt combined with a model and an output passer and these components are so-called runnables think of Lang chain expression language as a declarative way of composing these so-called runnables into chains and I'll get into what a runnable is in a second here we have a step-by-step example of what a chain looks like we have an input to the chain which is typically a dictionary that is sent to a prompt then a prompt value is sent to the or the chat model then the chat model will return a chat message that we want to pass and we typically want to use the passer to extract a string from the chat message and this will be the output of the chain this is one example of a basic chain and L chain allows us to chain multiple chains together like this so we might want to use the string output of the chain and send that to a new chain that will return a different output so what are runnables let's try to understand the runnable protocol this definition is directly from Lang chain a runnable is a unit of work that can be invoked batched streamed transformed and composed and the chains we built with Lang chain and the components of those chains are runnables and apart from Lang chain native components being runnables we can actually also pass in random functions into chain and those functions will then turn into runnables I'll show you that in an example when we get to the code and these chains can be invoked that means we execute the chain we can call batch on them if we have multiple inputs and we can stream the chains there are some core runnable objects in Lang chain that you want to understand in order to fully appreciate the Lang chain expression language these four objects are the most important ones runnable sequence runnable Lambda runnable pass through and runnable parallel so let's start from the top a runnable sequence in Lang chain is a class that chains together multiple runnable components and it ensures that each component processes its input and sequentially passes its output to the next component in general the change you're building in Lang chain will be of type runnable sequence now what about runnable Lambda runnable Lambda is a class that turns a python callable like a function into a runnable component and this allows us to integrate arbitary functions into chains as in the example I just showed you then we have runnable pass through and a runnable pass through in Lang chain is a class that either passes its input through unchanged or adds additional keys to the output in the first case it acts as a placeholder and in the second case it allows us to do flexible Integrations into sequences where we need to modify the input and finally we have runnable parallel and a runnable parallel in L chain is a class that runs multiple runnables concurrently and this allows you to do branching where you have two chains run the same input but return different outputs now let's see how this works by looking at some code examples all right so I'm going to use the same symbol example throughout this section I'm going to have an llm summarize AI Concepts here I have a prompt that tells the llm to summarize an AI concept and then I'm going to inject a context into the prompt the first thing I'm going to do is I'm going to create a simple chain using the pipe operator we already did it once now I'm going to add an additional element the output passer so the chain is composed of three elements the summarized prompt the llm and the output passer and when we invoke the chain we get the text output now remember that chains are of type runnable sequence and we can check the type of the chain that we just created to see if that is the case so I'm just going to print the type of the chain and here we can see that it is indeed of type runnable sequence now let's try to use a runnable Lambda here we have the same summarized chain as above but now with an additional element so I have a function that will calculate the length of the summary and then I'm wrapping this function in a runable Lambda this allows me to extend the chain we had before with an additional element so by adding this runnable Lambda to the chain from before I now have a chain that will return the length of the summary instead of the summary itself and here I can also check the type of the last element of the chain that I just added and it's of type runnable Lambda now we don't actually need to wrap the function in a runnable Lambda we can just add the function to the chain and then langin will make sure that the function is automatically converted to a runnable Lambda if I print the type of chain with function you can see that the last step of this chain is of runnable Lambda even though we didn't explicitly wrap the function in a runnable Lambda and let me just demonstrate that this chain actually works so I'm just going to invoke chain with function with the same context and here we get the summary length let's have a look at runnable pass through and first we'll go through the case where we use it as a placeholder I'm just going to instantiate a pass through and then I'm going to injected into the chain just before the length Lambda function and and as you can see we get the same output as before I can also check the types of the steps and you can see that the last step is still a runnable Lambda and the step before that is a runnable pass through passing data through without changing it like this is typically used with runnable parallel we can also use a runnable pass through for assigning value to the state of the chain to do that you will use the assign method of the runnable pass through and here I'm going to add the length of the summary to the state of the chain if I then add the assigned pass through as an additional component to the chain the output of the chain will now have an additional key with the length of the summary assigning an additional key value like this is particularly useful when you're working with SQL chains in some of my earlier videos on building custom SQL chains I'm actually using the assign method like this if I run this you can see that we now have the summary and the length and if I check the type of the last component of the chain I now get the type runnable assign the last example I'm going to give is the example of a runable parallel and in this case I will simply take the summary that the summarized chain is returning and then I'm going to Branch Out The Chain in two different arms one is going to take the summary and return the summary as it is this in the other one is going to return the length of the summary and this is going to give me something that is similar to what we got in the previous example so here my output contains the summary and the length of the summary and if I check the type of the last step of the chain I get a runnable parallel now let's talk about splits and retrievers I'm going to walk you through what a retriever is how we can split data into chunks and how we can load those chunks into a vctor store and in the cab notebook I'm going to give you a Hands-On example of working with a vector store I'm going to show you how you can use redis to build a vector store backed Retriever and do retrieval from Lang chain but let's start with answering the question what is a retriever a retriever in Lang chain is an interface case that returns documents given a query and this is not to be confused with a vector store so a vector store can be used as the backbone of a retriever but a retriever does not store the documents itself Vector store back retrievers are by far the most common retrievers when we build llm applications and when we build rag applications retrieval augmented generation applications we typically want to chunk up our data or split our data with a text splitter and store that in the vector store and langin has several built-in text Splitters that can be used for various different purposes it has a recursive text splitter that will try to keep the related chunks of text next to each other it has splitters for HTML markdown different types of code characters and tokens and I'll show you in a bit how to use the recursive text splitter when we get to the notebook so when the data has been chunked up into pieces we loaded into a vector store and then we can use that Vector store as the bases for a retriever that can be used in Chains or agents and if you go back and watch some of my earlier videos there are plenty of examples of using reddis and there's also a few examples of using pine cone but there are many different Vector databases to choose from and it's easy to switch up the vector database if you're using Lang chain because the interface is almost identical for for all the databases all right so let's head over to the collab notebook and see how we can work with Vector stores and how to use Splitters and retrievers so in this example I'm going to be using reddis as the vector store so I've pip installed redis and then I'm going to use the YouTube loader that we used before to extract the transcript from one of my videos here the loader will return a list of documents containing one document with the entire transcript of my video and now I can spit the text in the document using the recursive character text spitter and this text splitter is imported from langin text spitters I can then call the split documents method on the lists of documents containing a single document and then it will return a list of documents with chunks of text I could also have split the raw text by using the split text method of the text splitter once I have the chunks of text I want to load to a vector store I can connect to reddish or any other Vector database that Lang chain supports I'm going to need the host port and password in order to connect to redish and then the full reddish URL which is a combination of the three if you want to connect to redus through Lang chain and we want to do that here to use a hosted version of Reddit like I'm doing here just head over to reddit.com and sign up there a free tier and it's really easy to get started if you want to connect to reddis outside of L chain you can use reddish Pi so here I'm going to import reddis and then I'm connecting to reddis directly through redis py using the host port and password and once the connection has been established I can ping it to see that I actually have a connection and then I'm just going to flush the database to make sure it's empty next next up we're going to need some embeddings and I'm going to use hugging face embeddings since they are free so you'll need to pip install sentence Transformers I already did that here and then you can import hugging face embeddings from Lang chain embeddings and instantiate the embeddings it's going to take a few seconds to import this and when you're done you are ready to load the data to reddis I'm going to import Lang Chain's interface to redus because I want to load the vector data with Lang chain but I could also have done this manually using R pi and again I have an entire video covering that so just head over to the YouTube channel it should be easy to find so to load the vector data to radus we'll use the from documents method that's built into Lang chain Rus interface and as arguments we're going to pass in the list of documents with the chunks of text the embeddings the hugging phas embeddings and the reddish URL and then we're going to give it an index name let's call it YouTube and under the hood Lang chain is using the hogging face embeddings to create numeric vectors out of the text chunks and then load that to the vector database and once we have the data in the vector store we can set up the Retriever and again we use Lang chains radius object and we just call the method s Retriever with a search type and some keyword arguments and now we actually set up to do Vector similarity search over the documents in redish and just like we do with chains if we want to call the retriever we invoke it with a quiry and here we get all the chunks of data that are related in some way to data analysis and this list will contain exactly 10 documents as that's the parameter I set in the search keyword arguments so now we have a retriever and from here it's pretty easy to build a rack chain now let's move on to Rack retrieval augmented generation what is rack so rack is a technique for augmenting at l m knowledge with external data and a rag application is built in two steps the first one is indexing the external data that's what we did in the last section and in the second step we retrieve and generate the output with the llm so you already saw how the first step works we take some documents we chunk them up we turn them into numeric vectors and then we Lo them to a Victor database like reddish and the second step we retrieve and generate so the user query is used to fetch the context from the vector store and then that context is sent to the llm along side with the query itself and this gives the llm what it needs to generate a contextual output in a moment I'm going to show you how to build a rack chain and Lang chain using the retriever we just set up in the last section but before I do that I quickly want to make a comment on something I've noticed that a lot of AI practitioners have been thinking about do we really need to worry about this can we abstract away retrieval when we build chat Bots and rag applications if you're familiar with the OM assistance API you'll know that a lot of this is abstracted away when you build assistance that will do retrieval from a file and on the surface it looks like openi just removed a huge headache and made it a lot easier to build rag applications without having to worry about building retrievers but there is nothing simple about serving the right context to an llm based chatbot so if you're trying to build a chatbot that will convert customers for instance you're really facing a search problem and you generally have two metrics that you want to optimize precision and recall recall measures the completeness of the search results so how many of the relevant documents are we actually fetching and stuffing into the context window and precision measures the quality of the batch we are retrieving so the density of the relevant search results and in order to optimize the conversion rates in the chatbot you need both of these metrics and to do that you need control over the retrieval process which you don't have if you abstract away the retriever so just think about how many businesses are currently doing C conversion rate optimization on web pages and as more and more businesses are implementing chatbots and chat-based interfaces a lot of the C work is going to be focused on optimizing retrievers so I wouldn't bet on abstracting away retrievers at least not in the long run now let's build a rag chain using our reddish based retriever for this example I'm going to use a chat promp template and I'm injecting a context and the users's question into the template then I'm going to use a string output passer that allows me to extract the text output from the llm to set up the chain I'm going to use Lang chain expression language and in the first I'm extracting the question from the input data using the Lambda function and then passing the question to the retriever to get the context so the context here is a composition of two elements and then I also need to pass the question itself to the LM and I do that in the second line and then I use the pipe operator to assemble the entire chain consisting of the first dictionary The Prompt the llm and the string output passer and that's it now we have a rag chain we can then invoke the chain to get an answer to the question what can we do with llama 3 and this answer will then be based on the context which is given by the documents fetched by the retriever moving on to tools so what is a tool what is a toolkit and how do we use a tool tools and Lang chain are interfaces that agents chains or llms can use to interact with the world and I think a good way to think about tools is that just like humans need tools to perform tasks llms need tools as well for instance if you want to do some complex calculation that you can't do in your head you would need a tool for that a calculator or a computer program and similarly for the llm to be able to perform a task it will need access to computational resources or additional data in Lang chain you have several built-in tools you have the python Rebel tool you have a Wikipedia Tool YouTube Tool Saia tool and gradi Tool and so on and more and more tools are constantly being developed Lang chain also has built-in tool kits that are essentially collections of tools that are designed to be used to solve a specific task and examples of toolkits includes the air toolkit GitHub toolkit G lab toolkit and so on and just like tools the number of tool kits available for Lang chain is ever increasing so how do we use a tool and there are essentially two direct ways we can use a tool one is that we can directly run the tool with a query and just like loaders this will typically return some data or we can bind the tools to an llm and then we get an llm with access to tools that we can use in Chains and agents now let me show you how to do that by building on our YouTube example here in the notebook I'm going to pip install YouTube search and then I'm going to import YouTube search tool from Lang chain Community tools and the YouTube search tool allows me to search for YouTube videos so I'll in State the tool and then I can run the tool with the query rabid metric and as you can see this is going to return a list of videos which are videos from my YouTube channel now I can take this tool and I can bind it to the llm and then I get an llm with tools and now I can invoke the llm with tools with a query if I do that you can see that I get an AI message with some search arguments I can extract tool calls and that will give me a clear overview of the arguments and these are the arguments that you need to pass to the YouTube Tool now I can use a Lambda function to extract the arguments and then I can compose a chain consisting of my llm with tools the Lambda function and my YouTube Tool and this allows me to invoke the chain with random queres about YouTube videos so if I ask find some rabid Metric videos on Lang chain you can see that the chain is now returning me the list of YouTube videos so this is not much different than just running the search but there is now some llm based reasoning behind the crew and this is going to be more interesting once we start building agents finally let's talk about agents I'm going to to explain what an agent is and then we're going to build a YouTube summary agent what are agents so agents are programs capable of reasoning and Performing tasks by integrating llms with tools and they do so by leveraging the lm's capability to generate Json or other structured output this would be a very general definition of the concept in Lang chain agents have a very specific meaning in Lang chain an agent is a class that uses an llm to choose a sequence of actions to take the main conceptual difference between an agent and a chain is that in a chain the sequence of actions are hardcoded whereas in an agent we let the LM decide on what actions to take and what tools to use so think of the chain as a fixed sequence of steps whereas the agent might loop back and use the llm to decide on using a new tool and take a new action and this will go on until the llm decides that the task is completed in theory the agents are more flexible and robust than the chains however we also know that LMS can hallucinate so it depends on the use case now let's build an agent out of our YouTube use case after pip installed Lang chain hop I'm going to use that to extract a pre-made prompt and I'm going to import hop from Lang chain I'm going to import agent executor from Lang chain agents and create tool calling agent from Lang chain agents and the pre-made prom I'm going to use is available on the Hub and there you can see the template that's being used and then there's a variable Called Agent scratch pad and the agent scratch Pad is an important variable it is where the agent will save its intermediary steps but this prompt is ready to use along with our YouTube Tool and the llm and these are the only three things we need to create the agent so I'm going to pass the llm the tools the prompt to create tool calling agent and then I'll set up the agent executor and just like we did with the chains I can now invoke the agent executor and I'm going to ask the agent to find some Lang chain YouTube videos and here we see the list that it extracts with the tool and then it ranks the videos and finishes the chain and if we try to click on the first video we see that it gets Lang chain expained in 13 minutes by rabid metric what a brilliant agent now the cool thing about an agent is that we can just give it access to multiple tools and it will figure out on its own how to use the tools I'm now going to create a custom transcribe video tool using the tool decorator and the YouTube loader we used in section two and this tool will take a video URL and return the transcript of the video as a document then I'm going to create a list of tools with two tools the YouTube search tool which just used and the transcribe video tool and of course you need to have both YouTube search and the YouTube transcript API pip installed to do this and now we can invoke the agent executor and ask what topics the rabbit metric YouTube channel covers the YouTube agent will now use both tools add its disposal to give me a summary of the topics covered on the channel and here we have the final output with the summary of the topics covered all right that's it for now if you want to dive deeper into Lang chain I suggest you go check out some of the more advanced tutorials on the channel and if you enjoyed this tutorial and want to follow along for more content like this give it a like And subscribe thanks for watching\")]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5521d571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"in this video you'll learn everything you need to know about Lang chain in seven easy steps I've broken down the framework into key components and here's an overview of the image I'm going to try to put in your mind in the next 40 minutes there is a lot to unpack but I'll walk you through the components one by one and I've included coding examples with every step I'll start off by asking why length chain and I'll give you a reason why you want to learn this and an overview of the framework then I'll show you how to build chains using prompts and ERS and we'll dive into the runable protocol and Lang chain expression language then we'll look at Splitters and retrievers and we'll build a rag chain finally I'll walk you through how to use tools and how to build an agent with access to Tools in this video I'll keep my focus on the core Lang chain framework and build the foundation for advanced use cases so I'll save Lang graph Lang Smith and Lang surf for separate videos if you're new to Lang chain this tutorial will serve as a quick start and if you're already already familiar with the framework you can use the tutorial to tie up some loose ends I'll share a link to this interactive map of Lang chain to the code and to a written tutorial below this video let's get started I'm going to start off by giving you an overview of the linkchain framework what is it why would you want to use it and then I'm going to show you how to get started developing in a collab notebook so what is Lang chain Lang chain is an open open source framework for developing llm applications an engine allows developers to combine llms with external sources of computation and data and the framework further simplifies the deployment of the llm applications by providing the needed tools such as tools for inspection monitoring and evaluation the deployment and monitoring tools are part of Lang Smith and Lang serf I'm not going to cover that here I'm going to cover the core Library so Lang chain combines llms with external sources of computation and data how does that work suppose that a user wants to ask a question related to some documents or some data that the user has access to now this question will be sent to the llm just like when you're interacting with chat DBT but you also need to fetch the context needed to answer the question from your own data or your own documents and to do that a vector representation of the initial question from the user is used to fetch all the relevant context needed to answer the question typically from a vector database this context is then fed to the llm alongside with the initial question and now the llm has everything it needs to answer the question in the right context and the magical thing about this is that we can do more than just answer the question we can actually take actions based on the input from the end user and we can do that because Lang chain allows us to build agents out of llms and agents are created by by combining llms with tools this will give the llms access to external sources of computation and data so what can we build with Lang chain when it comes to the types of applications you can build with llms this guise the limit from what I see most llm applications that have been built over the last year fall into one or three categories chatbots racun applications or agent systems so the chat BS are nothing new they're used for marketing customer support different types of education and creating assistance what is new is that these chat Bots can now be made more intelligent using llms then we have the rag Q&A applications and this is the magic that Lang chain and llama index made people aware of a year ago so these are the applications that are used for summarizing large documents doing data analysis and generating code by referencing repositories and then we have what we can call agent system both multi-agent systems and systems where we Loop humans into complex interactions with agents and this is what Lang graph is used for and Lang graph is a rather new component in the Lang chain ecosystem the number of potential applications of agent systems is virtually Limitless they're being implemented for Supply Chain management Revenue optimization operations management and so on all right so let's zoom out a bit and have a look at why you want to learn langing so so there are at least two good reasons for learning Lang chain so first of all Lang chain facilitates the transitioning to the next generation of programming languages and what do I mean by that so if we have a look at programming language hierarchies this is what it typically looks like so at the bottom of the pyramid you will have machine language on top of that you will have Assembly Language then you have the higher level languages those are the languages that you are used to program coming in so python Java SQL R C++ and so on and we've been moving up the pyramid ever since programming became a thing and right now we are at the final step the highest level of all languages which is natural language because with the llms we don't have to write code anymore we can have the llms write code for us by giving natural language as an input and the development of these pipelines that allows us to turn natural language into programs is exactly what Lang chain facilitates so we can create pipelines or chains as they are called that allows us to input natural language and get something of value in return like a report an analysis or computer program and with the current generation of best-in-class llms we can already write executable code code that works so this is not something that belongs to the Future it's already here now the second reason you you might want to use Lang chain is really not Lang chain itself but what's being built on top of Lang chain which is Lang graph now Lang chain was late to the multi-agent game you had autogen you had crew AI before that but what langra can do is really remarkable for instance Lang graph allows us to build complex interactions between agents and humans and this is needed especially in big organizations where you have to guard the access to certain files and so on but Lang graph also allows us to build multi-agent systems like autogen or crew II and the use cases for this are becoming more clear every day we need to be able to set up arbitrary communication patterns between agents we also need to do divide and conquer just like humans we can't have an llm do everything so we want to build specialized agents and we want them to be able to perform tasks in an asynchronous way all right let's get started developing the first thing you want to do is you want to fetch the API key from the llm service you want to use so in this tutorial I'm going to use openi and anthropic but Lang chain supports a wide array of llm providers so you can just choose your favorite one once you have grabbed your API key you want to put it in a EnV file all right so here on the collab notebook I have already installed the needed libraries here I'm going to install python. EnV to fetch API keys from the EnV file I'm going to install Lang chain of course Lang chain Community Omi and anthropic and I have uploaded the EnV file from my local machine to the cab notebook and now I'm ready to load the environment variables so I'm loading the API keys and once I've done that I can use chat anthropic and chat Omi to connect to anthropic and Omi and then I'm going to make a test call to GPT 4 and ask what is Lang chain and I simply make an LM call by using the invoke method and here we have an answer from gbt 4 this is very similar to what you would do if you were just calling gbt 4 directly through the open API and of course we can just switch out the llm now I'm calling clot 3 and here I also get a response so I can also easily prpt the llm with a system message and a user message at the same time and I'll just send the two messages in a list to the llm and this will allow you to do some basic prompt engineering but we can do a lot more advanced stuff than this once we start utilizing prom templates and building chains all right so let's dive into chains prompts and loaders and build our first chain I'm going to walk you through what a chain is and give you an analogy to think of then I'm going to work you through prom templates and how to use document loaders so a chain in Lang chain is a sequence of interconnected components that process a user's query utilizing one or more llms to generate and deliver some valuable output and I'm going to get into what those components are in a bit but let me just give you examples of the valuable output that we can generate with the chain so examples of valuable output could be an answer to a specific question it could be a summary of some document it could be a recommendation insights from data a piece of executable code or some Json for an API call and with the chains we can automate the generation of this value now what about the components the components of a chain are typically proms llms or what we call chat models output passes that passes the output of the llm tools that allow the llms to extract additional information from apis or run some code this is what turns llms into agents and general functions and in Lang chain these components are so-called runnables that can be chained together to produce a pipeline and this is one way to think of chains as pipelines in the world of llms so an analogy would be a data pipeline in a data pipeline we typically input some raw data and we get some nice clean data out in a table format and the data cleaning and transformation happening in the pipeline is valuable similarly in an llm chain we input a query and we get some valuable output in return and here the query is turned into value using llm calls functions and additional data and just like with data pipelines we can automate this process with cicd continuous integration and continuous delivery and this is what Lang chain as a framework can help you do so not only can you build the chains but you can also test the chains deploy them monitor the chains and evaluate the output and this can be done with Lang Smith and Lang surf so Lang chain has several built-in modules or classes that helps you create chains efficiently and one of those modules are prompt templates so a prompt for an llm is a set of instructions provided by a user to guide the model's response and promt templates in Lang chain are predefined prompt recipes these recipes are very helpful when you want to create chains fast another feature of Lang chain is that you have so-called document loaders a document is a piece of text and Associated metadata and the Lang chain document loaders provide a load method for loading data as documents from a source into a chain now let's head over to a collab notebook and set up a simple chain I'm going to import prompt template from Lang chain prompts and then I'm going to formulate the prompt to the llm as a string with a placeholder in this case with the variable topic and then I set up the prompt using the prompt template I just imported from Lang chain an input variable that I'm going to call topic and then the string the template itself and this is a simple example of a prompt that now allows me to comp OS a chain and I do that using the pipe operator so I can take the prompt and the language model LL mgbt 4 and assemble a chain using those two components and now I can invoke the chain with an input variable in this case topic so this was a very simple example of assembling a chain using the pipe operator let's try to make this a bit more advanced I'm going to create a chain that will transcribe one of my YouTube videos so I'm going to pip install the YouTube transcript API and then I can use one of the loaders that is spilled into L chain the YouTube loader so I'll import YouTube loader from Lang chain Community document loaders and then I can simply pass the URL to one of my videos or any video to the loader and have the loader extract the transcript from the video now if we have a look at what the loader extracts you can see that we get a list of documents if you want the transcript you can simply extract the page content from the documents or you can simply pass the documents to the chain directly so fetching the raw transcript would be as simple as just fetching the single document in this list and then extracting the page content all right so let's set up a chain that will summarize YouTube videos given a transcript so here I'm going to inject the video transcript into the prom template so this is going going to be my input variable and then I just set up the chain like I did before with the pipe operator and I'm going to invoke it with the video transcript and note that I can just pass the documents I don't have to extract the raw transcript and here we have a nice summary of my YouTube video so Lang chain also has a number of build-in chains that you can use for stuff like this for instance the create stuff documents chain and this chain will take a list of documents and pass them all in as context in the prom template pretty much what we did before just without using the buil-in chain this was just a very basic introduction to building chains now in order to build more advanced chains we need to understand Lang chains runnable protocol and Lang chain expression language in more detail let's now try to dive deeper into Lang chain expression language and the runnable protocol the Lang chain expression language simplifies building complex chains from basic components and we do that using the pipe operator that allows us to chain different components together feeding the output from one component to the next so a simple example of a chain composed in this way would be a prompt combined with a model and an output passer and these components are so-called runnables think of Lang chain expression language as a declarative way of composing these so-called runnables into chains and I'll get into what a runnable is in a second here we have a step-by-step example of what a chain looks like we have an input to the chain which is typically a dictionary that is sent to a prompt then a prompt value is sent to the or the chat model then the chat model will return a chat message that we want to pass and we typically want to use the passer to extract a string from the chat message and this will be the output of the chain this is one example of a basic chain and L chain allows us to chain multiple chains together like this so we might want to use the string output of the chain and send that to a new chain that will return a different output so what are runnables let's try to understand the runnable protocol this definition is directly from Lang chain a runnable is a unit of work that can be invoked batched streamed transformed and composed and the chains we built with Lang chain and the components of those chains are runnables and apart from Lang chain native components being runnables we can actually also pass in random functions into chain and those functions will then turn into runnables I'll show you that in an example when we get to the code and these chains can be invoked that means we execute the chain we can call batch on them if we have multiple inputs and we can stream the chains there are some core runnable objects in Lang chain that you want to understand in order to fully appreciate the Lang chain expression language these four objects are the most important ones runnable sequence runnable Lambda runnable pass through and runnable parallel so let's start from the top a runnable sequence in Lang chain is a class that chains together multiple runnable components and it ensures that each component processes its input and sequentially passes its output to the next component in general the change you're building in Lang chain will be of type runnable sequence now what about runnable Lambda runnable Lambda is a class that turns a python callable like a function into a runnable component and this allows us to integrate arbitary functions into chains as in the example I just showed you then we have runnable pass through and a runnable pass through in Lang chain is a class that either passes its input through unchanged or adds additional keys to the output in the first case it acts as a placeholder and in the second case it allows us to do flexible Integrations into sequences where we need to modify the input and finally we have runnable parallel and a runnable parallel in L chain is a class that runs multiple runnables concurrently and this allows you to do branching where you have two chains run the same input but return different outputs now let's see how this works by looking at some code examples all right so I'm going to use the same symbol example throughout this section I'm going to have an llm summarize AI Concepts here I have a prompt that tells the llm to summarize an AI concept and then I'm going to inject a context into the prompt the first thing I'm going to do is I'm going to create a simple chain using the pipe operator we already did it once now I'm going to add an additional element the output passer so the chain is composed of three elements the summarized prompt the llm and the output passer and when we invoke the chain we get the text output now remember that chains are of type runnable sequence and we can check the type of the chain that we just created to see if that is the case so I'm just going to print the type of the chain and here we can see that it is indeed of type runnable sequence now let's try to use a runnable Lambda here we have the same summarized chain as above but now with an additional element so I have a function that will calculate the length of the summary and then I'm wrapping this function in a runable Lambda this allows me to extend the chain we had before with an additional element so by adding this runnable Lambda to the chain from before I now have a chain that will return the length of the summary instead of the summary itself and here I can also check the type of the last element of the chain that I just added and it's of type runnable Lambda now we don't actually need to wrap the function in a runnable Lambda we can just add the function to the chain and then langin will make sure that the function is automatically converted to a runnable Lambda if I print the type of chain with function you can see that the last step of this chain is of runnable Lambda even though we didn't explicitly wrap the function in a runnable Lambda and let me just demonstrate that this chain actually works so I'm just going to invoke chain with function with the same context and here we get the summary length let's have a look at runnable pass through and first we'll go through the case where we use it as a placeholder I'm just going to instantiate a pass through and then I'm going to injected into the chain just before the length Lambda function and and as you can see we get the same output as before I can also check the types of the steps and you can see that the last step is still a runnable Lambda and the step before that is a runnable pass through passing data through without changing it like this is typically used with runnable parallel we can also use a runnable pass through for assigning value to the state of the chain to do that you will use the assign method of the runnable pass through and here I'm going to add the length of the summary to the state of the chain if I then add the assigned pass through as an additional component to the chain the output of the chain will now have an additional key with the length of the summary assigning an additional key value like this is particularly useful when you're working with SQL chains in some of my earlier videos on building custom SQL chains I'm actually using the assign method like this if I run this you can see that we now have the summary and the length and if I check the type of the last component of the chain I now get the type runnable assign the last example I'm going to give is the example of a runable parallel and in this case I will simply take the summary that the summarized chain is returning and then I'm going to Branch Out The Chain in two different arms one is going to take the summary and return the summary as it is this in the other one is going to return the length of the summary and this is going to give me something that is similar to what we got in the previous example so here my output contains the summary and the length of the summary and if I check the type of the last step of the chain I get a runnable parallel now let's talk about splits and retrievers I'm going to walk you through what a retriever is how we can split data into chunks and how we can load those chunks into a vctor store and in the cab notebook I'm going to give you a Hands-On example of working with a vector store I'm going to show you how you can use redis to build a vector store backed Retriever and do retrieval from Lang chain but let's start with answering the question what is a retriever a retriever in Lang chain is an interface case that returns documents given a query and this is not to be confused with a vector store so a vector store can be used as the backbone of a retriever but a retriever does not store the documents itself Vector store back retrievers are by far the most common retrievers when we build llm applications and when we build rag applications retrieval augmented generation applications we typically want to chunk up our data or split our data with a text splitter and store that in the vector store and langin has several built-in text Splitters that can be used for various different purposes it has a recursive text splitter that will try to keep the related chunks of text next to each other it has splitters for HTML markdown different types of code characters and tokens and I'll show you in a bit how to use the recursive text splitter when we get to the notebook so when the data has been chunked up into pieces we loaded into a vector store and then we can use that Vector store as the bases for a retriever that can be used in Chains or agents and if you go back and watch some of my earlier videos there are plenty of examples of using reddis and there's also a few examples of using pine cone but there are many different Vector databases to choose from and it's easy to switch up the vector database if you're using Lang chain because the interface is almost identical for for all the databases all right so let's head over to the collab notebook and see how we can work with Vector stores and how to use Splitters and retrievers so in this example I'm going to be using reddis as the vector store so I've pip installed redis and then I'm going to use the YouTube loader that we used before to extract the transcript from one of my videos here the loader will return a list of documents containing one document with the entire transcript of my video and now I can spit the text in the document using the recursive character text spitter and this text splitter is imported from langin text spitters I can then call the split documents method on the lists of documents containing a single document and then it will return a list of documents with chunks of text I could also have split the raw text by using the split text method of the text splitter once I have the chunks of text I want to load to a vector store I can connect to reddish or any other Vector database that Lang chain supports I'm going to need the host port and password in order to connect to redish and then the full reddish URL which is a combination of the three if you want to connect to redus through Lang chain and we want to do that here to use a hosted version of Reddit like I'm doing here just head over to reddit.com and sign up there a free tier and it's really easy to get started if you want to connect to reddis outside of L chain you can use reddish Pi so here I'm going to import reddis and then I'm connecting to reddis directly through redis py using the host port and password and once the connection has been established I can ping it to see that I actually have a connection and then I'm just going to flush the database to make sure it's empty next next up we're going to need some embeddings and I'm going to use hugging face embeddings since they are free so you'll need to pip install sentence Transformers I already did that here and then you can import hugging face embeddings from Lang chain embeddings and instantiate the embeddings it's going to take a few seconds to import this and when you're done you are ready to load the data to reddis I'm going to import Lang Chain's interface to redus because I want to load the vector data with Lang chain but I could also have done this manually using R pi and again I have an entire video covering that so just head over to the YouTube channel it should be easy to find so to load the vector data to radus we'll use the from documents method that's built into Lang chain Rus interface and as arguments we're going to pass in the list of documents with the chunks of text the embeddings the hugging phas embeddings and the reddish URL and then we're going to give it an index name let's call it YouTube and under the hood Lang chain is using the hogging face embeddings to create numeric vectors out of the text chunks and then load that to the vector database and once we have the data in the vector store we can set up the Retriever and again we use Lang chains radius object and we just call the method s Retriever with a search type and some keyword arguments and now we actually set up to do Vector similarity search over the documents in redish and just like we do with chains if we want to call the retriever we invoke it with a quiry and here we get all the chunks of data that are related in some way to data analysis and this list will contain exactly 10 documents as that's the parameter I set in the search keyword arguments so now we have a retriever and from here it's pretty easy to build a rack chain now let's move on to Rack retrieval augmented generation what is rack so rack is a technique for augmenting at l m knowledge with external data and a rag application is built in two steps the first one is indexing the external data that's what we did in the last section and in the second step we retrieve and generate the output with the llm so you already saw how the first step works we take some documents we chunk them up we turn them into numeric vectors and then we Lo them to a Victor database like reddish and the second step we retrieve and generate so the user query is used to fetch the context from the vector store and then that context is sent to the llm along side with the query itself and this gives the llm what it needs to generate a contextual output in a moment I'm going to show you how to build a rack chain and Lang chain using the retriever we just set up in the last section but before I do that I quickly want to make a comment on something I've noticed that a lot of AI practitioners have been thinking about do we really need to worry about this can we abstract away retrieval when we build chat Bots and rag applications if you're familiar with the OM assistance API you'll know that a lot of this is abstracted away when you build assistance that will do retrieval from a file and on the surface it looks like openi just removed a huge headache and made it a lot easier to build rag applications without having to worry about building retrievers but there is nothing simple about serving the right context to an llm based chatbot so if you're trying to build a chatbot that will convert customers for instance you're really facing a search problem and you generally have two metrics that you want to optimize precision and recall recall measures the completeness of the search results so how many of the relevant documents are we actually fetching and stuffing into the context window and precision measures the quality of the batch we are retrieving so the density of the relevant search results and in order to optimize the conversion rates in the chatbot you need both of these metrics and to do that you need control over the retrieval process which you don't have if you abstract away the retriever so just think about how many businesses are currently doing C conversion rate optimization on web pages and as more and more businesses are implementing chatbots and chat-based interfaces a lot of the C work is going to be focused on optimizing retrievers so I wouldn't bet on abstracting away retrievers at least not in the long run now let's build a rag chain using our reddish based retriever for this example I'm going to use a chat promp template and I'm injecting a context and the users's question into the template then I'm going to use a string output passer that allows me to extract the text output from the llm to set up the chain I'm going to use Lang chain expression language and in the first I'm extracting the question from the input data using the Lambda function and then passing the question to the retriever to get the context so the context here is a composition of two elements and then I also need to pass the question itself to the LM and I do that in the second line and then I use the pipe operator to assemble the entire chain consisting of the first dictionary The Prompt the llm and the string output passer and that's it now we have a rag chain we can then invoke the chain to get an answer to the question what can we do with llama 3 and this answer will then be based on the context which is given by the documents fetched by the retriever moving on to tools so what is a tool what is a toolkit and how do we use a tool tools and Lang chain are interfaces that agents chains or llms can use to interact with the world and I think a good way to think about tools is that just like humans need tools to perform tasks llms need tools as well for instance if you want to do some complex calculation that you can't do in your head you would need a tool for that a calculator or a computer program and similarly for the llm to be able to perform a task it will need access to computational resources or additional data in Lang chain you have several built-in tools you have the python Rebel tool you have a Wikipedia Tool YouTube Tool Saia tool and gradi Tool and so on and more and more tools are constantly being developed Lang chain also has built-in tool kits that are essentially collections of tools that are designed to be used to solve a specific task and examples of toolkits includes the air toolkit GitHub toolkit G lab toolkit and so on and just like tools the number of tool kits available for Lang chain is ever increasing so how do we use a tool and there are essentially two direct ways we can use a tool one is that we can directly run the tool with a query and just like loaders this will typically return some data or we can bind the tools to an llm and then we get an llm with access to tools that we can use in Chains and agents now let me show you how to do that by building on our YouTube example here in the notebook I'm going to pip install YouTube search and then I'm going to import YouTube search tool from Lang chain Community tools and the YouTube search tool allows me to search for YouTube videos so I'll in State the tool and then I can run the tool with the query rabid metric and as you can see this is going to return a list of videos which are videos from my YouTube channel now I can take this tool and I can bind it to the llm and then I get an llm with tools and now I can invoke the llm with tools with a query if I do that you can see that I get an AI message with some search arguments I can extract tool calls and that will give me a clear overview of the arguments and these are the arguments that you need to pass to the YouTube Tool now I can use a Lambda function to extract the arguments and then I can compose a chain consisting of my llm with tools the Lambda function and my YouTube Tool and this allows me to invoke the chain with random queres about YouTube videos so if I ask find some rabid Metric videos on Lang chain you can see that the chain is now returning me the list of YouTube videos so this is not much different than just running the search but there is now some llm based reasoning behind the crew and this is going to be more interesting once we start building agents finally let's talk about agents I'm going to to explain what an agent is and then we're going to build a YouTube summary agent what are agents so agents are programs capable of reasoning and Performing tasks by integrating llms with tools and they do so by leveraging the lm's capability to generate Json or other structured output this would be a very general definition of the concept in Lang chain agents have a very specific meaning in Lang chain an agent is a class that uses an llm to choose a sequence of actions to take the main conceptual difference between an agent and a chain is that in a chain the sequence of actions are hardcoded whereas in an agent we let the LM decide on what actions to take and what tools to use so think of the chain as a fixed sequence of steps whereas the agent might loop back and use the llm to decide on using a new tool and take a new action and this will go on until the llm decides that the task is completed in theory the agents are more flexible and robust than the chains however we also know that LMS can hallucinate so it depends on the use case now let's build an agent out of our YouTube use case after pip installed Lang chain hop I'm going to use that to extract a pre-made prompt and I'm going to import hop from Lang chain I'm going to import agent executor from Lang chain agents and create tool calling agent from Lang chain agents and the pre-made prom I'm going to use is available on the Hub and there you can see the template that's being used and then there's a variable Called Agent scratch pad and the agent scratch Pad is an important variable it is where the agent will save its intermediary steps but this prompt is ready to use along with our YouTube Tool and the llm and these are the only three things we need to create the agent so I'm going to pass the llm the tools the prompt to create tool calling agent and then I'll set up the agent executor and just like we did with the chains I can now invoke the agent executor and I'm going to ask the agent to find some Lang chain YouTube videos and here we see the list that it extracts with the tool and then it ranks the videos and finishes the chain and if we try to click on the first video we see that it gets Lang chain expained in 13 minutes by rabid metric what a brilliant agent now the cool thing about an agent is that we can just give it access to multiple tools and it will figure out on its own how to use the tools I'm now going to create a custom transcribe video tool using the tool decorator and the YouTube loader we used in section two and this tool will take a video URL and return the transcript of the video as a document then I'm going to create a list of tools with two tools the YouTube search tool which just used and the transcribe video tool and of course you need to have both YouTube search and the YouTube transcript API pip installed to do this and now we can invoke the agent executor and ask what topics the rabbit metric YouTube channel covers the YouTube agent will now use both tools add its disposal to give me a summary of the topics covered on the channel and here we have the final output with the summary of the topics covered all right that's it for now if you want to dive deeper into Lang chain I suggest you go check out some of the more advanced tutorials on the channel and if you enjoyed this tutorial and want to follow along for more content like this give it a like And subscribe thanks for watching\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript = docs[0].page_content\n",
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff506863",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a helpful assistant that explains YT videos, Given the following trasncript, give a short summary.\n",
    "{video_transcript}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27f73ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"video_transcript\"],\n",
    "    template= prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fde4f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cff5e445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response  =  chain.invoke( {\"video_transcript\" : transcript } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f884513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_markdown_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee2068",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb7190a",
   "metadata": {},
   "source": [
    "# LCEL & Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c944dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "summarise_prompt_template = \"\"\"\n",
    "You are a helpful assistant that summarizes AI concepts:\n",
    "{context}\n",
    "Summarise the context\n",
    "\"\"\"\n",
    "\n",
    "summarize_prompt = PromptTemplate.from_template(summarise_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7893a5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant that summarizes AI concepts:\\n{context}\\nSummarise the context\\n')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28231175",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = summarize_prompt | gemma | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13c13f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke( {\"context\" : \"What is Lanchain?\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed9fb8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ffb25cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, let's break down LaunchChain.\n",
       "\n",
       "**Context & Summary of LaunchChain**\n",
       "\n",
       "LaunchChain is a decentralized launchpad built on the Binance Smart Chain (BSC). Essentially, it's a platform designed to help new cryptocurrency projects raise capital by offering a more accessible and potentially fairer alternative to traditional centralized launchpads.\n",
       "\n",
       "Here’s a breakdown of the key things to know:\n",
       "\n",
       "*   **Decentralized Launchpad:** Unlike traditional launchpads run by centralized exchanges, LaunchChain leverages blockchain technology. This means decisions about which projects are listed and how funds are distributed are made through a decentralized voting system.\n",
       "*   **Token Generation Event (TGE):** LaunchChain facilitates TGEs, where new crypto tokens are created and sold to investors.\n",
       "*   **Community-Driven Voting:**  Token holders (typically those who have staked LaunchChain's native token, LAUNCH) vote on which projects they want to see listed on the launchpad. This aims to reduce the influence of large investors and promote projects that have genuine community support.\n",
       "*   **Revenue Sharing:**  LaunchChain earns a percentage of the funds raised during TGEs, which is then distributed to LAUNCH token holders. This rewards the community for their participation.\n",
       "*   **Staking Rewards:**  Users can earn rewards by staking their LAUNCH tokens, further incentivizing engagement with the platform.\n",
       "*   **BSC Focused:** LaunchChain is specifically designed for projects launching on the Binance Smart Chain.\n",
       "\n",
       "\n",
       "**In short:** LaunchChain is a decentralized launchpad that lets the community vote on and fund new crypto projects, offering a more democratic and potentially lucrative way for both projects and investors to participate in the blockchain space.\n",
       "\n",
       "---\n",
       "\n",
       "**Do you want me to delve deeper into any specific aspect of LaunchChain, such as:**\n",
       "\n",
       "*   How the voting system works?\n",
       "*   How revenue is shared?\n",
       "*   The risks and potential rewards of using LaunchChain?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_markdown_text(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c459ae1",
   "metadata": {},
   "source": [
    "### Runnable Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2a1269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f1cfa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_chain = summarize_prompt | gemma | output_parser\n",
    "\n",
    "length_lambda = RunnableLambda(lambda summary: f\"Summary length: {len(summary)}\" )\n",
    "\n",
    "lambda_chain = summarize_chain | length_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34ee22db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_chain.invoke( {\"context\" : \"What are lambda functions in python?\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "938e47f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type( lambda_chain.steps[-1] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd988903",
   "metadata": {},
   "source": [
    "#### without converting to runnable lambda beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8e9f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_function = summarize_chain | (lambda summary: f\"Summary length: {len(summary)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "156a81e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type( chain_with_function.steps[-1] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befbe0d8",
   "metadata": {},
   "source": [
    "### Runnable Passthrough as placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e794fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ab3fea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "passthrough = RunnablePassthrough()\n",
    "placeholder_chain = summarize_chain | passthrough | length_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "675d9b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder_chain.invoke( {\"context\" : \"What is langchain?\"} )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0c6612",
   "metadata": {},
   "source": [
    "#### Runnable Passthrough for assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83012068",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_summary_lambda = RunnableLambda( lambda summary: {\"summary\" : summary} )\n",
    "\n",
    "assign_passthrough = RunnablePassthrough.assign( length=lambda x: len(x[\"summary\"]) )\n",
    "\n",
    "summarize_chain = summarize_prompt | gemma | output_parser | wrap_summary_lambda\n",
    "\n",
    "assign_chain = summarize_chain | assign_passthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c903e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign_chain.invoke( {\"context\" : \"Function of heart in 4 lines\"} )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177f277d",
   "metadata": {},
   "source": [
    "### Runnable Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "53d1a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f711944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_chain = summarize_prompt | gemma | output_parser\n",
    "\n",
    "parallel_runnable = RunnableParallel(\n",
    "    summary = lambda x: x,\n",
    "    length = lambda x: len(x)\n",
    ")\n",
    "\n",
    "parallel_chain = summarize_chain | parallel_runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0dd61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel_chain.invoke( {\"context\" : \"What is a graph?\" } )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e8e66e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226f8c28",
   "metadata": {},
   "source": [
    "# Splitters & Retreivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "29b640ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "701526cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=8BV9TW490nQ&t=533s\", add_video_info = False\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51969531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "676459e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9032a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_split = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81ad4bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"in this video you'll learn everything you need to know about Lang chain in seven easy steps I've\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"easy steps I've broken down the framework into key components and here's an overview of the image\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"of the image I'm going to try to put in your mind in the next 40 minutes there is a lot to unpack\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"is a lot to unpack but I'll walk you through the components one by one and I've included coding\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"included coding examples with every step I'll start off by asking why length chain and I'll give\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"chain and I'll give you a reason why you want to learn this and an overview of the framework then\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the framework then I'll show you how to build chains using prompts and ERS and we'll dive into the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"we'll dive into the runable protocol and Lang chain expression language then we'll look at\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"then we'll look at Splitters and retrievers and we'll build a rag chain finally I'll walk you\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"I'll walk you through how to use tools and how to build an agent with access to Tools in this video\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"Tools in this video I'll keep my focus on the core Lang chain framework and build the foundation\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the foundation for advanced use cases so I'll save Lang graph Lang Smith and Lang surf for separate\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"surf for separate videos if you're new to Lang chain this tutorial will serve as a quick start and\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"a quick start and if you're already already familiar with the framework you can use the tutorial to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"use the tutorial to tie up some loose ends I'll share a link to this interactive map of Lang chain\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"map of Lang chain to the code and to a written tutorial below this video let's get started I'm\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"get started I'm going to start off by giving you an overview of the linkchain framework what is it\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"what is it why would you want to use it and then I'm going to show you how to get started\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='how to get started developing in a collab notebook so what is Lang chain Lang chain is an open open'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='is an open open source framework for developing llm applications an engine allows developers to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='developers to combine llms with external sources of computation and data and the framework further'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='framework further simplifies the deployment of the llm applications by providing the needed tools'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the needed tools such as tools for inspection monitoring and evaluation the deployment and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the deployment and monitoring tools are part of Lang Smith and Lang serf I'm not going to cover\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"not going to cover that here I'm going to cover the core Library so Lang chain combines llms with\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='combines llms with external sources of computation and data how does that work suppose that a user'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='suppose that a user wants to ask a question related to some documents or some data that the user'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"data that the user has access to now this question will be sent to the llm just like when you're\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"like when you're interacting with chat DBT but you also need to fetch the context needed to answer\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='needed to answer the question from your own data or your own documents and to do that a vector'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to do that a vector representation of the initial question from the user is used to fetch all the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to fetch all the relevant context needed to answer the question typically from a vector database'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='a vector database this context is then fed to the llm alongside with the initial question and now'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='question and now the llm has everything it needs to answer the question in the right context and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='right context and the magical thing about this is that we can do more than just answer the question'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='answer the question we can actually take actions based on the input from the end user and we can do'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='user and we can do that because Lang chain allows us to build agents out of llms and agents are'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='llms and agents are created by by combining llms with tools this will give the llms access to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the llms access to external sources of computation and data so what can we build with Lang chain'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='with Lang chain when it comes to the types of applications you can build with llms this guise the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='llms this guise the limit from what I see most llm applications that have been built over the last'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='built over the last year fall into one or three categories chatbots racun applications or agent'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"or agent systems so the chat BS are nothing new they're used for marketing customer support\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='customer support different types of education and creating assistance what is new is that these'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='new is that these chat Bots can now be made more intelligent using llms then we have the rag Q&A'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='we have the rag Q&A applications and this is the magic that Lang chain and llama index made people'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='index made people aware of a year ago so these are the applications that are used for summarizing'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='for summarizing large documents doing data analysis and generating code by referencing repositories'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='repositories and then we have what we can call agent system both multi-agent systems and systems'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='systems and systems where we Loop humans into complex interactions with agents and this is what'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and this is what Lang graph is used for and Lang graph is a rather new component in the Lang chain'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='in the Lang chain ecosystem the number of potential applications of agent systems is virtually'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"is virtually Limitless they're being implemented for Supply Chain management Revenue optimization\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"optimization operations management and so on all right so let's zoom out a bit and have a look at\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and have a look at why you want to learn langing so so there are at least two good reasons for'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='good reasons for learning Lang chain so first of all Lang chain facilitates the transitioning to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='transitioning to the next generation of programming languages and what do I mean by that so if we'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='by that so if we have a look at programming language hierarchies this is what it typically looks'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='it typically looks like so at the bottom of the pyramid you will have machine language on top of'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='language on top of that you will have Assembly Language then you have the higher level languages'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='level languages those are the languages that you are used to program coming in so python Java SQL R'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"python Java SQL R C++ and so on and we've been moving up the pyramid ever since programming became\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='programming became a thing and right now we are at the final step the highest level of all'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"level of all languages which is natural language because with the llms we don't have to write code\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='have to write code anymore we can have the llms write code for us by giving natural language as an'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='language as an input and the development of these pipelines that allows us to turn natural language'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='natural language into programs is exactly what Lang chain facilitates so we can create pipelines or'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='create pipelines or chains as they are called that allows us to input natural language and get'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='language and get something of value in return like a report an analysis or computer program and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='program and with the current generation of best-in-class llms we can already write executable code'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"executable code code that works so this is not something that belongs to the Future it's already\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"Future it's already here now the second reason you you might want to use Lang chain is really not\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"chain is really not Lang chain itself but what's being built on top of Lang chain which is Lang\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='chain which is Lang graph now Lang chain was late to the multi-agent game you had autogen you had'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='had autogen you had crew AI before that but what langra can do is really remarkable for instance'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='for instance Lang graph allows us to build complex interactions between agents and humans and this'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and humans and this is needed especially in big organizations where you have to guard the access to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='guard the access to certain files and so on but Lang graph also allows us to build multi-agent'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='build multi-agent systems like autogen or crew II and the use cases for this are becoming more'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='are becoming more clear every day we need to be able to set up arbitrary communication patterns'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"patterns between agents we also need to do divide and conquer just like humans we can't have an llm\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"can't have an llm do everything so we want to build specialized agents and we want them to be able\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"them to be able to perform tasks in an asynchronous way all right let's get started developing the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='developing the first thing you want to do is you want to fetch the API key from the llm service you'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the llm service you want to use so in this tutorial I'm going to use openi and anthropic but Lang\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='anthropic but Lang chain supports a wide array of llm providers so you can just choose your'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='just choose your favorite one once you have grabbed your API key you want to put it in a EnV file'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='it in a EnV file all right so here on the collab notebook I have already installed the needed'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the needed libraries here I'm going to install python. EnV to fetch API keys from the EnV file I'm\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the EnV file I'm going to install Lang chain of course Lang chain Community Omi and anthropic and I\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and anthropic and I have uploaded the EnV file from my local machine to the cab notebook and now'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"notebook and now I'm ready to load the environment variables so I'm loading the API keys and once\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"API keys and once I've done that I can use chat anthropic and chat Omi to connect to anthropic and\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"to anthropic and Omi and then I'm going to make a test call to GPT 4 and ask what is Lang chain and\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='is Lang chain and I simply make an LM call by using the invoke method and here we have an answer'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='we have an answer from gbt 4 this is very similar to what you would do if you were just calling gbt'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='just calling gbt 4 directly through the open API and of course we can just switch out the llm now'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"out the llm now I'm calling clot 3 and here I also get a response so I can also easily prpt the llm\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"easily prpt the llm with a system message and a user message at the same time and I'll just send\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and I'll just send the two messages in a list to the llm and this will allow you to do some basic\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to do some basic prompt engineering but we can do a lot more advanced stuff than this once we start'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"this once we start utilizing prom templates and building chains all right so let's dive into chains\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"dive into chains prompts and loaders and build our first chain I'm going to walk you through what a\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"you through what a chain is and give you an analogy to think of then I'm going to work you through\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to work you through prom templates and how to use document loaders so a chain in Lang chain is a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"in Lang chain is a sequence of interconnected components that process a user's query utilizing one\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"query utilizing one or more llms to generate and deliver some valuable output and I'm going to get\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"I'm going to get into what those components are in a bit but let me just give you examples of the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='you examples of the valuable output that we can generate with the chain so examples of valuable'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of valuable output could be an answer to a specific question it could be a summary of some document'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of some document it could be a recommendation insights from data a piece of executable code or some'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='code or some Json for an API call and with the chains we can automate the generation of this value'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of this value now what about the components the components of a chain are typically proms llms or'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='proms llms or what we call chat models output passes that passes the output of the llm tools that'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the llm tools that allow the llms to extract additional information from apis or run some code this'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='run some code this is what turns llms into agents and general functions and in Lang chain these'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='in Lang chain these components are so-called runnables that can be chained together to produce a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to produce a pipeline and this is one way to think of chains as pipelines in the world of llms so'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='world of llms so an analogy would be a data pipeline in a data pipeline we typically input some raw'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='input some raw data and we get some nice clean data out in a table format and the data cleaning and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='data cleaning and transformation happening in the pipeline is valuable similarly in an llm chain we'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='in an llm chain we input a query and we get some valuable output in return and here the query is'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='here the query is turned into value using llm calls functions and additional data and just like'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='data and just like with data pipelines we can automate this process with cicd continuous'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='cicd continuous integration and continuous delivery and this is what Lang chain as a framework can'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='as a framework can help you do so not only can you build the chains but you can also test the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='can also test the chains deploy them monitor the chains and evaluate the output and this can be'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and this can be done with Lang Smith and Lang surf so Lang chain has several built-in modules or'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='built-in modules or classes that helps you create chains efficiently and one of those modules are'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='those modules are prompt templates so a prompt for an llm is a set of instructions provided by a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"provided by a user to guide the model's response and promt templates in Lang chain are predefined\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='are predefined prompt recipes these recipes are very helpful when you want to create chains fast'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='create chains fast another feature of Lang chain is that you have so-called document loaders a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='document loaders a document is a piece of text and Associated metadata and the Lang chain document'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='Lang chain document loaders provide a load method for loading data as documents from a source into'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"from a source into a chain now let's head over to a collab notebook and set up a simple chain I'm\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"a simple chain I'm going to import prompt template from Lang chain prompts and then I'm going to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"then I'm going to formulate the prompt to the llm as a string with a placeholder in this case with\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='in this case with the variable topic and then I set up the prompt using the prompt template I just'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"template I just imported from Lang chain an input variable that I'm going to call topic and then\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='call topic and then the string the template itself and this is a simple example of a prompt that'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of a prompt that now allows me to comp OS a chain and I do that using the pipe operator so I can'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='operator so I can take the prompt and the language model LL mgbt 4 and assemble a chain using those'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='a chain using those two components and now I can invoke the chain with an input variable in this'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='variable in this case topic so this was a very simple example of assembling a chain using the pipe'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"using the pipe operator let's try to make this a bit more advanced I'm going to create a chain that\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"create a chain that will transcribe one of my YouTube videos so I'm going to pip install the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to pip install the YouTube transcript API and then I can use one of the loaders that is spilled'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"that is spilled into L chain the YouTube loader so I'll import YouTube loader from Lang chain\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='from Lang chain Community document loaders and then I can simply pass the URL to one of my videos'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to one of my videos or any video to the loader and have the loader extract the transcript from the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='transcript from the video now if we have a look at what the loader extracts you can see that we get'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='can see that we get a list of documents if you want the transcript you can simply extract the page'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='extract the page content from the documents or you can simply pass the documents to the chain'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to the chain directly so fetching the raw transcript would be as simple as just fetching the single'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"fetching the single document in this list and then extracting the page content all right so let's\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"all right so let's set up a chain that will summarize YouTube videos given a transcript so here I'm\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"so here I'm going to inject the video transcript into the prom template so this is going going to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='is going going to be my input variable and then I just set up the chain like I did before with the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"did before with the pipe operator and I'm going to invoke it with the video transcript and note\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"transcript and note that I can just pass the documents I don't have to extract the raw transcript\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the raw transcript and here we have a nice summary of my YouTube video so Lang chain also has a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='chain also has a number of build-in chains that you can use for stuff like this for instance the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='for instance the create stuff documents chain and this chain will take a list of documents and pass'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='documents and pass them all in as context in the prom template pretty much what we did before just'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='we did before just without using the buil-in chain this was just a very basic introduction to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='introduction to building chains now in order to build more advanced chains we need to understand'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='need to understand Lang chains runnable protocol and Lang chain expression language in more detail'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"in more detail let's now try to dive deeper into Lang chain expression language and the runnable\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and the runnable protocol the Lang chain expression language simplifies building complex chains'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='complex chains from basic components and we do that using the pipe operator that allows us to chain'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='allows us to chain different components together feeding the output from one component to the next'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to the next so a simple example of a chain composed in this way would be a prompt combined with a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='combined with a model and an output passer and these components are so-called runnables think of'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='runnables think of Lang chain expression language as a declarative way of composing these so-called'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"these so-called runnables into chains and I'll get into what a runnable is in a second here we have\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='second here we have a step-by-step example of what a chain looks like we have an input to the chain'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='input to the chain which is typically a dictionary that is sent to a prompt then a prompt value is'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='a prompt value is sent to the or the chat model then the chat model will return a chat message that'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='a chat message that we want to pass and we typically want to use the passer to extract a string'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to extract a string from the chat message and this will be the output of the chain this is one'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='chain this is one example of a basic chain and L chain allows us to chain multiple chains together'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='chains together like this so we might want to use the string output of the chain and send that to a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and send that to a new chain that will return a different output so what are runnables let's try to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"let's try to understand the runnable protocol this definition is directly from Lang chain a\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='from Lang chain a runnable is a unit of work that can be invoked batched streamed transformed and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='transformed and composed and the chains we built with Lang chain and the components of those chains'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of those chains are runnables and apart from Lang chain native components being runnables we can'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='runnables we can actually also pass in random functions into chain and those functions will then'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"functions will then turn into runnables I'll show you that in an example when we get to the code\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='we get to the code and these chains can be invoked that means we execute the chain we can call'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='chain we can call batch on them if we have multiple inputs and we can stream the chains there are'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='chains there are some core runnable objects in Lang chain that you want to understand in order to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='in order to fully appreciate the Lang chain expression language these four objects are the most'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='are the most important ones runnable sequence runnable Lambda runnable pass through and runnable'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and runnable parallel so let's start from the top a runnable sequence in Lang chain is a class that\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='is a class that chains together multiple runnable components and it ensures that each component'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='that each component processes its input and sequentially passes its output to the next component in'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"next component in general the change you're building in Lang chain will be of type runnable\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='be of type runnable sequence now what about runnable Lambda runnable Lambda is a class that turns a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='class that turns a python callable like a function into a runnable component and this allows us to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='this allows us to integrate arbitary functions into chains as in the example I just showed you then'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='showed you then we have runnable pass through and a runnable pass through in Lang chain is a class'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='chain is a class that either passes its input through unchanged or adds additional keys to the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='keys to the output in the first case it acts as a placeholder and in the second case it allows us'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='case it allows us to do flexible Integrations into sequences where we need to modify the input and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the input and finally we have runnable parallel and a runnable parallel in L chain is a class that'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='is a class that runs multiple runnables concurrently and this allows you to do branching where you'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"branching where you have two chains run the same input but return different outputs now let's see\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"now let's see how this works by looking at some code examples all right so I'm going to use the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"going to use the same symbol example throughout this section I'm going to have an llm summarize AI\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='an llm summarize AI Concepts here I have a prompt that tells the llm to summarize an AI concept and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"an AI concept and then I'm going to inject a context into the prompt the first thing I'm going to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"thing I'm going to do is I'm going to create a simple chain using the pipe operator we already did\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"we already did it once now I'm going to add an additional element the output passer so the chain is\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='so the chain is composed of three elements the summarized prompt the llm and the output passer and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='output passer and when we invoke the chain we get the text output now remember that chains are of'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='that chains are of type runnable sequence and we can check the type of the chain that we just'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"chain that we just created to see if that is the case so I'm just going to print the type of the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the type of the chain and here we can see that it is indeed of type runnable sequence now let's try\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"now let's try to use a runnable Lambda here we have the same summarized chain as above but now with\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='above but now with an additional element so I have a function that will calculate the length of the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the length of the summary and then I'm wrapping this function in a runable Lambda this allows me to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='this allows me to extend the chain we had before with an additional element so by adding this'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='so by adding this runnable Lambda to the chain from before I now have a chain that will return the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='will return the length of the summary instead of the summary itself and here I can also check the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"can also check the type of the last element of the chain that I just added and it's of type\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and it's of type runnable Lambda now we don't actually need to wrap the function in a runnable\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='in a runnable Lambda we can just add the function to the chain and then langin will make sure that'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='will make sure that the function is automatically converted to a runnable Lambda if I print the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='if I print the type of chain with function you can see that the last step of this chain is of'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"of this chain is of runnable Lambda even though we didn't explicitly wrap the function in a\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the function in a runnable Lambda and let me just demonstrate that this chain actually works so I'm\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"works so I'm just going to invoke chain with function with the same context and here we get the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and here we get the summary length let's have a look at runnable pass through and first we'll go\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and first we'll go through the case where we use it as a placeholder I'm just going to instantiate\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"to instantiate a pass through and then I'm going to injected into the chain just before the length\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='before the length Lambda function and and as you can see we get the same output as before I can'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='as before I can also check the types of the steps and you can see that the last step is still a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='step is still a runnable Lambda and the step before that is a runnable pass through passing data'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='passing data through without changing it like this is typically used with runnable parallel we can'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='parallel we can also use a runnable pass through for assigning value to the state of the chain to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"of the chain to do that you will use the assign method of the runnable pass through and here I'm\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and here I'm going to add the length of the summary to the state of the chain if I then add the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='if I then add the assigned pass through as an additional component to the chain the output of the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the output of the chain will now have an additional key with the length of the summary assigning an'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"assigning an additional key value like this is particularly useful when you're working with SQL\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"working with SQL chains in some of my earlier videos on building custom SQL chains I'm actually\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"chains I'm actually using the assign method like this if I run this you can see that we now have\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='that we now have the summary and the length and if I check the type of the last component of the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"component of the chain I now get the type runnable assign the last example I'm going to give is the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to give is the example of a runable parallel and in this case I will simply take the summary that'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the summary that the summarized chain is returning and then I'm going to Branch Out The Chain in\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='Out The Chain in two different arms one is going to take the summary and return the summary as it'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the summary as it is this in the other one is going to return the length of the summary and this is'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='summary and this is going to give me something that is similar to what we got in the previous'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='got in the previous example so here my output contains the summary and the length of the summary'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of the summary and if I check the type of the last step of the chain I get a runnable parallel now'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"parallel now let's talk about splits and retrievers I'm going to walk you through what a retriever\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='what a retriever is how we can split data into chunks and how we can load those chunks into a vctor'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"chunks into a vctor store and in the cab notebook I'm going to give you a Hands-On example of\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"Hands-On example of working with a vector store I'm going to show you how you can use redis to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"can use redis to build a vector store backed Retriever and do retrieval from Lang chain but let's\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"chain but let's start with answering the question what is a retriever a retriever in Lang chain is\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='in Lang chain is an interface case that returns documents given a query and this is not to be'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='this is not to be confused with a vector store so a vector store can be used as the backbone of a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the backbone of a retriever but a retriever does not store the documents itself Vector store back'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='Vector store back retrievers are by far the most common retrievers when we build llm applications'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='llm applications and when we build rag applications retrieval augmented generation applications we'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='applications we typically want to chunk up our data or split our data with a text splitter and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='a text splitter and store that in the vector store and langin has several built-in text Splitters'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='text Splitters that can be used for various different purposes it has a recursive text splitter'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='text splitter that will try to keep the related chunks of text next to each other it has splitters'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"it has splitters for HTML markdown different types of code characters and tokens and I'll show you\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and I'll show you in a bit how to use the recursive text splitter when we get to the notebook so\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to the notebook so when the data has been chunked up into pieces we loaded into a vector store and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='a vector store and then we can use that Vector store as the bases for a retriever that can be used'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='that can be used in Chains or agents and if you go back and watch some of my earlier videos there'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"videos there are plenty of examples of using reddis and there's also a few examples of using pine\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"of using pine cone but there are many different Vector databases to choose from and it's easy to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and it's easy to switch up the vector database if you're using Lang chain because the interface is\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the interface is almost identical for for all the databases all right so let's head over to the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='head over to the collab notebook and see how we can work with Vector stores and how to use'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and how to use Splitters and retrievers so in this example I'm going to be using reddis as the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"using reddis as the vector store so I've pip installed redis and then I'm going to use the YouTube\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to use the YouTube loader that we used before to extract the transcript from one of my videos here'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of my videos here the loader will return a list of documents containing one document with the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='document with the entire transcript of my video and now I can spit the text in the document using'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the document using the recursive character text spitter and this text splitter is imported from'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='is imported from langin text spitters I can then call the split documents method on the lists of'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='on the lists of documents containing a single document and then it will return a list of documents'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='a list of documents with chunks of text I could also have split the raw text by using the split'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='by using the split text method of the text splitter once I have the chunks of text I want to load'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='text I want to load to a vector store I can connect to reddish or any other Vector database that'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"database that Lang chain supports I'm going to need the host port and password in order to connect\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='in order to connect to redish and then the full reddish URL which is a combination of the three if'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of the three if you want to connect to redus through Lang chain and we want to do that here to use'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"do that here to use a hosted version of Reddit like I'm doing here just head over to reddit.com and\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"to reddit.com and sign up there a free tier and it's really easy to get started if you want to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"if you want to connect to reddis outside of L chain you can use reddish Pi so here I'm going to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"here I'm going to import reddis and then I'm connecting to reddis directly through redis py using\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='redis py using the host port and password and once the connection has been established I can ping'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"I can ping it to see that I actually have a connection and then I'm just going to flush the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"going to flush the database to make sure it's empty next next up we're going to need some\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"going to need some embeddings and I'm going to use hugging face embeddings since they are free so\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"they are free so you'll need to pip install sentence Transformers I already did that here and then\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='that here and then you can import hugging face embeddings from Lang chain embeddings and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"embeddings and instantiate the embeddings it's going to take a few seconds to import this and when\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"this and when you're done you are ready to load the data to reddis I'm going to import Lang Chain's\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"import Lang Chain's interface to redus because I want to load the vector data with Lang chain but I\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='Lang chain but I could also have done this manually using R pi and again I have an entire video'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='an entire video covering that so just head over to the YouTube channel it should be easy to find so'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"be easy to find so to load the vector data to radus we'll use the from documents method that's\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"method that's built into Lang chain Rus interface and as arguments we're going to pass in the list\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to pass in the list of documents with the chunks of text the embeddings the hugging phas embeddings'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"phas embeddings and the reddish URL and then we're going to give it an index name let's call it\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"name let's call it YouTube and under the hood Lang chain is using the hogging face embeddings to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='face embeddings to create numeric vectors out of the text chunks and then load that to the vector'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='that to the vector database and once we have the data in the vector store we can set up the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='we can set up the Retriever and again we use Lang chains radius object and we just call the method'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='call the method s Retriever with a search type and some keyword arguments and now we actually set'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='now we actually set up to do Vector similarity search over the documents in redish and just like we'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and just like we do with chains if we want to call the retriever we invoke it with a quiry and here'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='a quiry and here we get all the chunks of data that are related in some way to data analysis and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"data analysis and this list will contain exactly 10 documents as that's the parameter I set in the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"I set in the search keyword arguments so now we have a retriever and from here it's pretty easy to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"it's pretty easy to build a rack chain now let's move on to Rack retrieval augmented generation\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='generation what is rack so rack is a technique for augmenting at l m knowledge with external data'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='with external data and a rag application is built in two steps the first one is indexing the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"one is indexing the external data that's what we did in the last section and in the second step we\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the second step we retrieve and generate the output with the llm so you already saw how the first'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='saw how the first step works we take some documents we chunk them up we turn them into numeric'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='them into numeric vectors and then we Lo them to a Victor database like reddish and the second step'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and the second step we retrieve and generate so the user query is used to fetch the context from'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the context from the vector store and then that context is sent to the llm along side with the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='along side with the query itself and this gives the llm what it needs to generate a contextual'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"a contextual output in a moment I'm going to show you how to build a rack chain and Lang chain\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and Lang chain using the retriever we just set up in the last section but before I do that I'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"before I do that I quickly want to make a comment on something I've noticed that a lot of AI\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='that a lot of AI practitioners have been thinking about do we really need to worry about this can'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"about this can we abstract away retrieval when we build chat Bots and rag applications if you're\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"if you're familiar with the OM assistance API you'll know that a lot of this is abstracted away\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='is abstracted away when you build assistance that will do retrieval from a file and on the surface'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and on the surface it looks like openi just removed a huge headache and made it a lot easier to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='it a lot easier to build rag applications without having to worry about building retrievers but'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='retrievers but there is nothing simple about serving the right context to an llm based chatbot so'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"based chatbot so if you're trying to build a chatbot that will convert customers for instance\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"for instance you're really facing a search problem and you generally have two metrics that you want\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='that you want to optimize precision and recall recall measures the completeness of the search'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of the search results so how many of the relevant documents are we actually fetching and stuffing'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and stuffing into the context window and precision measures the quality of the batch we are'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of the batch we are retrieving so the density of the relevant search results and in order to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and in order to optimize the conversion rates in the chatbot you need both of these metrics and to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"metrics and to do that you need control over the retrieval process which you don't have if you\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"don't have if you abstract away the retriever so just think about how many businesses are currently\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='are currently doing C conversion rate optimization on web pages and as more and more businesses are'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='more businesses are implementing chatbots and chat-based interfaces a lot of the C work is going to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"C work is going to be focused on optimizing retrievers so I wouldn't bet on abstracting away\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"on abstracting away retrievers at least not in the long run now let's build a rag chain using our\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"rag chain using our reddish based retriever for this example I'm going to use a chat promp template\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"chat promp template and I'm injecting a context and the users's question into the template then I'm\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"template then I'm going to use a string output passer that allows me to extract the text output\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the text output from the llm to set up the chain I'm going to use Lang chain expression language\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"expression language and in the first I'm extracting the question from the input data using the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='data using the Lambda function and then passing the question to the retriever to get the context so'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='get the context so the context here is a composition of two elements and then I also need to pass'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='I also need to pass the question itself to the LM and I do that in the second line and then I use'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='line and then I use the pipe operator to assemble the entire chain consisting of the first'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"of the first dictionary The Prompt the llm and the string output passer and that's it now we have a\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='it now we have a rag chain we can then invoke the chain to get an answer to the question what can'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='question what can we do with llama 3 and this answer will then be based on the context which is'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='context which is given by the documents fetched by the retriever moving on to tools so what is a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='tools so what is a tool what is a toolkit and how do we use a tool tools and Lang chain are'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and Lang chain are interfaces that agents chains or llms can use to interact with the world and I'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the world and I think a good way to think about tools is that just like humans need tools to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='need tools to perform tasks llms need tools as well for instance if you want to do some complex'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"to do some complex calculation that you can't do in your head you would need a tool for that a\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='a tool for that a calculator or a computer program and similarly for the llm to be able to perform'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='be able to perform a task it will need access to computational resources or additional data in Lang'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='data in Lang chain you have several built-in tools you have the python Rebel tool you have a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='tool you have a Wikipedia Tool YouTube Tool Saia tool and gradi Tool and so on and more and more'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and more and more tools are constantly being developed Lang chain also has built-in tool kits that'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='tool kits that are essentially collections of tools that are designed to be used to solve a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='be used to solve a specific task and examples of toolkits includes the air toolkit GitHub toolkit G'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='GitHub toolkit G lab toolkit and so on and just like tools the number of tool kits available for'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='kits available for Lang chain is ever increasing so how do we use a tool and there are essentially'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='are essentially two direct ways we can use a tool one is that we can directly run the tool with a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='run the tool with a query and just like loaders this will typically return some data or we can bind'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='data or we can bind the tools to an llm and then we get an llm with access to tools that we can use'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='that we can use in Chains and agents now let me show you how to do that by building on our YouTube'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"on our YouTube example here in the notebook I'm going to pip install YouTube search and then I'm\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"search and then I'm going to import YouTube search tool from Lang chain Community tools and the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"tools and the YouTube search tool allows me to search for YouTube videos so I'll in State the tool\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='in State the tool and then I can run the tool with the query rabid metric and as you can see this'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='as you can see this is going to return a list of videos which are videos from my YouTube channel'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='my YouTube channel now I can take this tool and I can bind it to the llm and then I get an llm with'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='I get an llm with tools and now I can invoke the llm with tools with a query if I do that you can'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='I do that you can see that I get an AI message with some search arguments I can extract tool calls'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='extract tool calls and that will give me a clear overview of the arguments and these are the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and these are the arguments that you need to pass to the YouTube Tool now I can use a Lambda'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='I can use a Lambda function to extract the arguments and then I can compose a chain consisting of'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='chain consisting of my llm with tools the Lambda function and my YouTube Tool and this allows me to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='this allows me to invoke the chain with random queres about YouTube videos so if I ask find some'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='if I ask find some rabid Metric videos on Lang chain you can see that the chain is now returning me'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='is now returning me the list of YouTube videos so this is not much different than just running the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='just running the search but there is now some llm based reasoning behind the crew and this is going'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and this is going to be more interesting once we start building agents finally let's talk about\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"let's talk about agents I'm going to to explain what an agent is and then we're going to build a\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='going to build a YouTube summary agent what are agents so agents are programs capable of reasoning'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of reasoning and Performing tasks by integrating llms with tools and they do so by leveraging the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"by leveraging the lm's capability to generate Json or other structured output this would be a very\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='would be a very general definition of the concept in Lang chain agents have a very specific meaning'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='specific meaning in Lang chain an agent is a class that uses an llm to choose a sequence of actions'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='sequence of actions to take the main conceptual difference between an agent and a chain is that in'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='a chain is that in a chain the sequence of actions are hardcoded whereas in an agent we let the LM'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='agent we let the LM decide on what actions to take and what tools to use so think of the chain as a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of the chain as a fixed sequence of steps whereas the agent might loop back and use the llm to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and use the llm to decide on using a new tool and take a new action and this will go on until the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='go on until the llm decides that the task is completed in theory the agents are more flexible and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='more flexible and robust than the chains however we also know that LMS can hallucinate so it'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"hallucinate so it depends on the use case now let's build an agent out of our YouTube use case\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"YouTube use case after pip installed Lang chain hop I'm going to use that to extract a pre-made\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"extract a pre-made prompt and I'm going to import hop from Lang chain I'm going to import agent\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to import agent executor from Lang chain agents and create tool calling agent from Lang chain'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"from Lang chain agents and the pre-made prom I'm going to use is available on the Hub and there you\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"Hub and there you can see the template that's being used and then there's a variable Called Agent\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='Called Agent scratch pad and the agent scratch Pad is an important variable it is where the agent'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='is where the agent will save its intermediary steps but this prompt is ready to use along with our'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='use along with our YouTube Tool and the llm and these are the only three things we need to create'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"we need to create the agent so I'm going to pass the llm the tools the prompt to create tool\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"to create tool calling agent and then I'll set up the agent executor and just like we did with the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"we did with the chains I can now invoke the agent executor and I'm going to ask the agent to find\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the agent to find some Lang chain YouTube videos and here we see the list that it extracts with the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='extracts with the tool and then it ranks the videos and finishes the chain and if we try to click'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='if we try to click on the first video we see that it gets Lang chain expained in 13 minutes by'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='in 13 minutes by rabid metric what a brilliant agent now the cool thing about an agent is that we'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='an agent is that we can just give it access to multiple tools and it will figure out on its own how'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"out on its own how to use the tools I'm now going to create a custom transcribe video tool using\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='video tool using the tool decorator and the YouTube loader we used in section two and this tool'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='two and this tool will take a video URL and return the transcript of the video as a document then'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"as a document then I'm going to create a list of tools with two tools the YouTube search tool which\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='search tool which just used and the transcribe video tool and of course you need to have both'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='need to have both YouTube search and the YouTube transcript API pip installed to do this and now we'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='do this and now we can invoke the agent executor and ask what topics the rabbit metric YouTube'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='metric YouTube channel covers the YouTube agent will now use both tools add its disposal to give me'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='disposal to give me a summary of the topics covered on the channel and here we have the final'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"we have the final output with the summary of the topics covered all right that's it for now if you\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='it for now if you want to dive deeper into Lang chain I suggest you go check out some of the more'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='some of the more advanced tutorials on the channel and if you enjoyed this tutorial and want to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and want to follow along for more content like this give it a like And subscribe thanks for'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='thanks for watching')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a6a14867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "910a6fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "85760a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568cd84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rds = Chroma.from_documents(\n",
    "#     documents=docs_split,\n",
    "#     embedding=embeddings,\n",
    "#     collection_name=\"my-collection\",\n",
    "#     persist_directory=\"./chroma_langchain_db\"   \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c6d623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = rds.as_retriever( search_type=\"similarity\", search_kwargs= {\"k\":10} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b844e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"in this video you'll learn everything you need to know about Lang chain in seven easy steps I've\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"in this video you'll learn everything you need to know about Lang chain in seven easy steps I've\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"in this video you'll learn everything you need to know about Lang chain in seven easy steps I've\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"use the tutorial to tie up some loose ends I'll share a link to this interactive map of Lang chain\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"use the tutorial to tie up some loose ends I'll share a link to this interactive map of Lang chain\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"use the tutorial to tie up some loose ends I'll share a link to this interactive map of Lang chain\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"Tools in this video I'll keep my focus on the core Lang chain framework and build the foundation\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"Tools in this video I'll keep my focus on the core Lang chain framework and build the foundation\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"Tools in this video I'll keep my focus on the core Lang chain framework and build the foundation\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"chain is really not Lang chain itself but what's being built on top of Lang chain which is Lang\")]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retriever.invoke( \"langchain\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3680e5be",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "609be30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "19468186",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "79f5ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "87d322cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df85141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain = (\n",
    "#     {\"context\" : (lambda x: x[\"question\"]) | retriever,\n",
    "#      \"question\" : (lambda x: x[\"question\"])}\n",
    "#     | prompt\n",
    "#     | gemma\n",
    "#     | output_parser\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d58d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chain.invoke({\"question\":\"What is ragchain?, elaborate in detail\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "46096aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Markdown((answer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593eec79",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3437777a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.10 environment at: C:\\Users\\Sarvamm\\AppData\\Local\\Programs\\Python\\Python312\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mIgnoring dangling temporary directory: `\u001b[36mC:\\Users\\Sarvamm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~otebook-7.4.0.dist-info\u001b[39m`\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m6 packages\u001b[0m \u001b[2min 582ms\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 53ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 23ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1myoutube-search\u001b[0m\u001b[2m==2.1.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%uv pip install --system youtube_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1063473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import YouTubeSearchTool\n",
    "youtube_tool = YouTubeSearchTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7e3dab4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_markdown_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mshow_markdown_text\u001b[49m(\u001b[38;5;28mstr\u001b[39m(youtube_tool.run(\u001b[33m\"\u001b[39m\u001b[33mMarkiplier\u001b[39m\u001b[33m\"\u001b[39m)))\n",
      "\u001b[31mNameError\u001b[39m: name 'show_markdown_text' is not defined"
     ]
    }
   ],
   "source": [
    "show_markdown_text(str(youtube_tool.run(\"Markiplier\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c409057",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen = ChatOllama(\n",
    "    model=\"qwen3:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f2b22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_with_tool = qwen.bind_tools([youtube_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8deae685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# msg = qwen_with_tool.invoke(\"markiplier youtube videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bec472ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_markdown_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mshow_markdown_text\u001b[49m(\u001b[38;5;28mstr\u001b[39m(msg))\n",
      "\u001b[31mNameError\u001b[39m: name 'show_markdown_text' is not defined"
     ]
    }
   ],
   "source": [
    "show_markdown_text(str(msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "372522c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'youtube_search',\n",
       "  'args': {'num_results': 5, 'query': 'Markiplier'},\n",
       "  'id': '5b1ebc34-f04b-410e-9e71-809a6011a275',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acd7fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = qwen_with_tool | (lambda x: x.tool_calls[0]['args']['query']) | youtube_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0294173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = chain.invoke(\"Find some rabbitmetrics videos on langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b19e3bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_markdown_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mshow_markdown_text\u001b[49m(\u001b[38;5;28mstr\u001b[39m(x))\n",
      "\u001b[31mNameError\u001b[39m: name 'show_markdown_text' is not defined"
     ]
    }
   ],
   "source": [
    "show_markdown_text(str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c0560",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "018ffe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21de25c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sarvamm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull('hwchase17/openai-tools-agent')\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a41d1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [youtube_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bf50b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_tool_calling_agent(qwen_with_tool, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90d342e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ba24440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sarvamm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ollama\\_types.py:81: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  if key in self.model_fields:\n",
      "c:\\Users\\Sarvamm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ollama\\_types.py:82: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  return self.model_fields[key].default is not None\n",
      "c:\\Users\\Sarvamm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ollama\\_types.py:81: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  if key in self.model_fields:\n",
      "c:\\Users\\Sarvamm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ollama\\_types.py:82: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  return self.model_fields[key].default is not None\n",
      "c:\\Users\\Sarvamm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ollama\\_types.py:81: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  if key in self.model_fields:\n",
      "c:\\Users\\Sarvamm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ollama\\_types.py:82: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  return self.model_fields[key].default is not None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `youtube_search` with `{'query': 'twerking tutorials,5'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['https://www.youtube.com/shorts/Ov1oFfDA-9Q', 'https://www.youtube.com/watch?v=t2Deg12uDrM&pp=ygUSdHdlcmtpbmcgdHV0b3JpYWxz', 'https://www.youtube.com/shorts/YnBZAGzH990', 'https://www.youtube.com/shorts/F3ZkasP9xS0', 'https://www.youtube.com/shorts/CpVdwBjNQZk']\u001b[0m\u001b[32;1m\u001b[1;3m<think>\n",
      "Okay, the user asked for twerking tutorials, so I need to provide them with some video links. Let me check the tool response first. The YouTube search function returned five links. I should present these in a clear way.\n",
      "\n",
      "Wait, maybe the user is looking for dance tutorials, so I should mention that these are dance videos. Also, I should format the links as clickable if possible. Let me list each link with a brief description. Oh, and maybe add a note about the number of results since there are five. Alright, that should cover it.\n",
      "</think>\n",
      "\n",
      "Here are some twerking tutorials from YouTube:\n",
      "\n",
      "1. [Twerking Dance Tutorial](https://www.youtube.com/shorts/Ov1oFfDA-9Q)  \n",
      "2. [Twerking Basics](https://www.youtube.com/watch?v=t2Deg12uDrM&pp=ygUSdHdlcmtpbmcgdHV0b3JpYWxz)  \n",
      "3. [Twerking Moves](https://www.youtube.com/shorts/YnBZAGzH990)  \n",
      "4. [Twerking Step-by-Step](https://www.youtube.com/shorts/F3ZkasP9xS0)  \n",
      "5. [Twerking Fun](https://www.youtube.com/shorts/CpVdwBjNQZk)  \n",
      "\n",
      "Let me know if you'd like more! 🕺💃\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sarvamm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ollama\\_types.py:81: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  if key in self.model_fields:\n",
      "c:\\Users\\Sarvamm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ollama\\_types.py:82: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  return self.model_fields[key].default is not None\n"
     ]
    }
   ],
   "source": [
    "x = agent_executor.invoke(\n",
    "    {\n",
    "        \"input\" : \"Find some twerking tutorials\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54226014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, the user asked for twerking tutorials, so I need to provide them with some video links. Let me check the tool response first. The YouTube search function returned five links. I should present these in a clear way.\n",
       "\n",
       "Wait, maybe the user is looking for dance tutorials, so I should mention that these are dance videos. Also, I should format the links as clickable if possible. Let me list each link with a brief description. Oh, and maybe add a note about the number of results since there are five. Alright, that should cover it.\n",
       "</think>\n",
       "\n",
       "Here are some twerking tutorials from YouTube:\n",
       "\n",
       "1. [Twerking Dance Tutorial](https://www.youtube.com/shorts/Ov1oFfDA-9Q)  \n",
       "2. [Twerking Basics](https://www.youtube.com/watch?v=t2Deg12uDrM&pp=ygUSdHdlcmtpbmcgdHV0b3JpYWxz)  \n",
       "3. [Twerking Moves](https://www.youtube.com/shorts/YnBZAGzH990)  \n",
       "4. [Twerking Step-by-Step](https://www.youtube.com/shorts/F3ZkasP9xS0)  \n",
       "5. [Twerking Fun](https://www.youtube.com/shorts/CpVdwBjNQZk)  \n",
       "\n",
       "Let me know if you'd like more! 🕺💃"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_markdown_text(str(x['output']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f82b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
